<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <style>
    /* Custom styles for the project */
body {
  font-family: Arial, sans-serif;
}

.container {
  max-width: 960px;
  margin: 0 auto;
  padding: 40px 20px;
}

.university-header {
  background-color: #5c0000;
  color: #ffffff;
  text-align: center;
  padding: 20px 0;
}

.university-header__title {
  font-size: 28px;
  margin: 0;
}

.university-header__description {
  font-size: 18px;
  margin-top: 10px;
}

.university-section {
  padding: 40px 0;
}

.university-section__title {
  font-size: 24px;
  margin-bottom: 20px;
}

.bg-light {
  background-color: #f7f7f7;
}

.row {
  display: flex;
  flex-wrap: wrap;
  margin-right: -15px;
  margin-left: -15px;
}

.col-2 {
  width: calc(16.6667% - 30px);
  margin-right: 15px;
  margin-left: 15px;
}

.col-12 {
  width: 100%;
  margin-right: 0;
  margin-left: 0;
}

.img-fluid {
  max-width: 100%;
  height: auto;
}

.university-footer {
  background-color: #5c0000;
  color: #ffffff;
  text-align: center;
  padding: 20px 0;
}

  </style>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CycleGAN Coloring Book Generator</title>
  <style>
    /* Custom styles for the project */
    /* Add your custom CSS here */
  </style>
</head>

<body>

  <header class="university-header">
    <div class="container">
      <h1 class="university-header__title">CycleGAN Coloring Book Generator</h1>
      <p class="university-header__description">Transform normal photos into coloring book versions using CycleGAN</p>
    </div>
  </header>

  <section class="university-section">
    <div class="container">
      <h2 class="university-section__title">Description</h2>
      <p>
        The CycleGAN Coloring Book Generator is a project that employs the CycleGAN architecture to transform normal photos into coloring book versions. By leveraging the power of CycleGAN, the project enables the conversion of images from one domain to another without the need for paired training data. The generator network learns to map images from the source domain to the target domain, while the discriminator network ensures the realism of the generated coloring book images. This project provides a user-friendly interface to facilitate the seamless generation of coloring book versions from user-supplied photographs.
      </p>
    </div>
  </section>

  <section class="university-section bg-light">
    <div class="container">
      <h2 class="university-section__title">Motivation</h2>
      <p>
        The coloring book generator serves as an offline stress reduction alternative, providing individuals with a creative outlet to unwind and relax. Engaging in coloring activities has been shown to have therapeutic benefits, allowing individuals to escape from everyday stressors and focus their attention on a calming and enjoyable task. By using the coloring book generator, users can easily transform normal photos into coloring book versions, which encourages artistic expression and helps improve fine motor skills. This process can be seen as a hobby, allowing individuals to engage in a pleasurable and rewarding activity during their leisure time. One of the notable advantages of using the coloring book generator is the sense of achievement it brings. As users witness the transformation of their favorite photos into coloring book pages, they experience a sense of accomplishment and satisfaction. This feeling of achievement can boost self-confidence and provide a positive emotional experience. Overall, the coloring book generator serves as an enjoyable and fulfilling activity that promotes stress reduction, encourages artistic expression, improves motor skills, and provides a sense of achievement. By engaging in this offline hobby, individuals can enhance their well-being and find a creative outlet for relaxation and self-expression.
      </p>
    </div>
  </section>

  <section class="university-section">
    <div class="container">
      <h2 class="university-section__title">Technologies</h2>
      <ul>
        <li>Python: Programming language used for development.</li>
        <li>PyTorch: Deep learning framework for training and implementing the CycleGAN model.</li>
        <li>OpenCV: Library for image processing and manipulation.</li>
        <li>GUI Framework (e.g., Tkinter): Used for creating the user interface.</li>
      </ul>
    </div>
  </section>

  <section class="university-section bg-light">
    <div class="container">
      <h2 class="university-section__title">Dataset</h2>
      <p>
        The dataset used in this project consists of two domains: Domain X and Domain Y.

        <li>Domain X:
          Domain X comprises real-world photos sourced from the Open Images dataset. 
          Open Images is a large-scale dataset containing millions of images from diverse categories. The real-world photos in Domain X capture a wide range of subjects, scenes, and objects, providing a rich and varied source of visual data.
          </li>
                <div class="col-2">
          <p>Open Images</p>
          <img src="images/opi.png" alt="2nd Try" title="2nd Try">
        </div>
<li>Domain Y:
Domain Y consists of sketches collected from museums around the world, specifically from the Imagenet-Sketch dataset. Imagenet-Sketch is a curated collection of hand-drawn sketches that represent different objects, animals, and scenes. These sketches have been created by artists and serve as a valuable resource for generating coloring book line drawings.
</li>
                    <div class="col-2">
          <p>Imagenet Sketch</p>
          <img src="images/imsk.png" alt="2nd Try" title="2nd Try">
        </div>
                    <div class="col-2">
          <p>Collected from Museums</p>
          <img src="images/custom.png" alt="2nd Try" title="2nd Try">
        </div>
By using these two distinct domains, the project aims to leverage the diversity and characteristics of both real-world photos and hand-drawn sketches to train the CycleGAN model. This unpaired dataset allows the model to learn the mapping between the two domains and generate coloring book versions of real-world photos without the need for directly paired training examples.
      </p>
    </div>
  </section>

  <section class="university-section bg-light">
    <div class="container">
      <h2 class="university-section__title">Results</h2>
      <div class="row">
  <ul>
    <li>
      <h3>First Try</h3>
      <ul>
        <li>Trained the model using 100 real elephant images in each style for 200 epochs.</li>
        <li>Incorporated dense adult style drawings with dark pixels defining object boundaries.</li>
        <li>Results: Generated images were somewhat fuzzy but showed good recognition of intricate details in darker images.</li>
      </ul>
    </li>
    <li>
      <h3>Second Try</h3>
      <ul>
        <li>Expanded the dataset in Domain X by including a wider range of animals and doubling the overall data size.</li>
        <li>Increased the number of dense adult style drawings to enhance stylistic transfer.</li>
        <li>Results: Specific results for this experiment are not available.</li>
      </ul>
    </li>
    <li>
      <h3>Third Try</h3>
      <ul>
        <li>Diversified the dataset by introducing scenic and other image types.</li>
        <li>Extended training duration to 300 epochs and doubled the number of style drawings.</li>
        <li>Introduced simple images with white backgrounds and thin object lines to handle minimalistic and line-based styles.</li>
      </ul>
    </li>
    <li>
      <h3>Fourth Try</h3>
      <ul>
        <li>Increased the dataset size to 500 images in both domains while maintaining the same ratio in Domain Y.</li>
        <li>Trained the model for 200 epochs to improve stylization results.</li>
      </ul>
    </li>
    <li>
      <h3>Fifth Try (Final Experiment)</h3>
      <ul>
        <li>Expanded the dataset significantly to 900 images in each domain, including a broad variety of real-world pictures in Domain X.</li>
        <li>Trained the model for 125 epochs to capture a wider range of stylistic elements and enhance overall performance.</li>
      </ul>
    </li>
  </ul>
  <p>Throughout these experiments, we aimed to refine the style transfer process and achieve improved results by adjusting the dataset composition, training duration, and the incorporation of different style elements.</p>
</div>


      <div class="row">
        <div class="col-2">
          <p>Input 256x256 pixels</p>
          <img src="images/input.png" alt="Input" title="Input">
        </div>
        <div class="col-2">
          <p>First Try </p>
          <img src="images/1t.png" alt="1st Try" title="1st Try">
        </div>
        <div class="col-2">
          <p>Second Try</p>
          <img src="images/2t.png" alt="2nd Try" title="2nd Try">
        </div>
        <div class="col-2">
          <p>Third Try</p>
          <img src="images/3t.png" alt="3rd Try" title="3rd Try">
        </div>
        <div class="col-2">
          <p>Fourth Try</p>
          <img src="images/4t.png" alt="4th Try" title="4th Try">
        </div>
      </div>
      <div class="row mt-4">
        <div class="col-12">
          <p>Final Try</p>
          <img src="images/5t.png" alt="Actual Results" title="Actual Results" class="img-fluid">
        </div>
      </div>
      
      <div class="row mt-4">
        <div class="col-12">
          <p>More Results, input color images and corresponding coloring book sketch</p>
          <img src="images/mr.png" alt="Actual Results" title="Actual Results" class="img-fluid">
        </div>
      </div>
      
      <div class="row mt-4">
        <div class="col-12">
                    <img src="images/rr.png" alt="Actual Results" title="Actual Results" class="img-fluid">
        </div>
      </div>
    </div>
  </section>


</body>

</html>
